# Can language models learn the style of a text author?
## Abstract 
The aim of this project is to evaluate if text generating language models are capable to learn the style of an author on which texts/speeches models were trained. To do this, definition of written style is analysed, defined the components. For text generation n-gram, LSTM based, and GPT-2 simple version language models were considered.  Style similarity was evaluated regarding quality using M-BLEU4 score and style strength using two trained classifiers. For first classifier naïve bayes, gradient boosting, SVM and logistic regression with word count vectorization input was considered. Highest accuracy was obtained with Naïve bayes classifier = 0.92. The second classifier is BiLSTM based where inputs are parts of speech tags and its dependencies, classifier obtained accuracy of 0.85. Using this metrics, language model generated texts were compared with original D. Trump rally speeches and other politician rally speeches using Mann-Whitney U test.  With a significance of 0.05, none of language models were capable to generate texts which would be similar to real D. Trump speeches in all three criteria. Thus, there is no evidence to say that in this project considered language models are capable to learn the style of the author. 
